# Apache Kafka Broadcaster and Client Specification (Beta)

## Status
Draft (Beta)

## Purpose
Define **Kafka broadcaster** (source) and **Kafka client** (sink) behavior for:
- CDC and write-after log (WAL) streaming
- Audit event delivery
- Ingestion and migration pipelines

This spec is intended to be implementable without external references.

## Scope
This specification covers:
- Kafka producer/consumer roles in ScratchBird
- Topic naming and payload formats
- Delivery semantics and idempotency
- Security configuration and credential handling

Non-goals:
- Kafka broker deployment or cluster operations
- Full Kafka Connect runtime (only connector expectations)

## Components

### 1) Kafka Broadcaster (Source)
Publishes events generated by the ScratchBird engine:
- **Change events (CDC)**: row-level DML changes
- **DDL events**: schema changes (optional channel)
- **Audit events**: security and policy auditing

### 2) Kafka Client (Sink)
Consumes Kafka topics and applies or ingests data:
- **CDC ingest** into ScratchBird tables
- **Migration replay** for cutover workflows
- **Audit rehydration** (optional)

### 3) Kafka Connect Adapters (Optional)
ScratchBird provides specs for:
- **Source connector**: emits CDC and DDL events
- **Sink connector**: applies events to ScratchBird

## Event Streams and Formats

### A) Change Events (CDC)
Use the change event envelope defined in:
`docs/specifications/Alpha Phase 2/04-Replication-Protocol-Specification.md`

**Default encoding**: MessagePack  
**Optional encodings**: Protobuf or JSON

**Minimum envelope fields**
- `event_id` (UUID)
- `tx_id` (UUID)
- `commit_ts_utc` (int64 microseconds)
- `schema_id` (UUID)
- `table_id` (UUID)
- `row_id` (UUID)
- `operation` (INSERT/UPDATE/DELETE)
- `payload_before` (optional)
- `payload_after` (optional)

**Topic defaults**
- `scratchbird.cdc.<db_uuid>`
- `scratchbird.cdc.<db_uuid>.<table_uuid>`

### B) DDL Events (Schema Changes)
DDL events are optional but recommended for downstream consumers.

**Minimum envelope fields**
- `event_id` (UUID)
- `tx_id` (UUID)
- `commit_ts_utc` (int64)
- `schema_id` (UUID)
- `ddl_sql` (string)
- `object_ids` (list of UUIDs)
- `event_type` (CREATE/ALTER/DROP)

**Topic defaults**
- `scratchbird.ddl.<db_uuid>`

### C) Audit Events
Audit events follow the Security Audit specification:
`docs/specifications/Security Design Specification/08_AUDIT_COMPLIANCE.md`

**Topic defaults**
- `scratchbird.audit.<category>`

## Delivery Semantics

### Producer Guarantees (Broadcaster)
- **At-least-once delivery** is the default.
- Idempotency is enabled by `event_id` + `tx_id`.
- Ordering is per partition (by `table_id` or `row_id`).

### Consumer Guarantees (Client)
- **Idempotent apply**: duplicate events are ignored by `event_id`.
- Consumers persist offsets in catalog metadata to allow safe restart.
- Replay is supported by retaining CDC topics for configured intervals.

## Topic Partitioning

Recommended keys:
- `table_id` for table-ordering semantics
- `row_id` for per-row ordering semantics

Default strategy:
- CDC topics partition by `table_id`
- Audit topics partition by `category`

## Configuration

### Core Kafka Settings
```
[kafka]
bootstrap_servers = "kafka1:9092,kafka2:9092"
client_id = "scratchbird"
acks = "all"
compression = "lz4"
security_protocol = "SASL_SSL"
sasl_mechanism = "SCRAM-SHA-512"
sasl_username = "..."
sasl_password = "..."
ssl_ca = "/etc/ssl/certs/ca.pem"
```

### CDC Broadcaster Settings
```
[kafka.cdc]
enabled = true
topic_prefix = "scratchbird.cdc"
partition_key = "table_id"
format = "messagepack"  # messagepack|protobuf|json
retention = "72h"
```

### CDC Client (Ingest) Settings
```
[kafka.cdc_ingest]
enabled = true
group_id = "scratchbird-cdc-ingest"
topic = "scratchbird.cdc.<db_uuid>"
apply_mode = "idempotent"
```

### Audit Kafka Settings
Use the existing audit configuration keys:
```
audit.kafka.enabled = true
audit.kafka.brokers = "kafka1:9092,kafka2:9092"
audit.kafka.topic_prefix = "scratchbird.audit"
audit.kafka.security_protocol = "SASL_SSL"
audit.kafka.sasl_mechanism = "SCRAM-SHA-512"
```

### Replication Channel Settings (Kafka WAL)
Kafka settings for WAL streaming are defined in:
`docs/specifications/beta_requirements/replication/REPLICATION_AND_SHADOW_PROTOCOLS.md`

Example parameters:
```
wal_channel = "KAFKA"
kafka_brokers = "kafka1:9092,kafka2:9092"
kafka_topic = "scratchbird.wal.prod"
kafka_compression = "LZ4"
```

## Security Requirements
- TLS is required for production use.
- SASL/SCRAM is recommended for authentication.
- Credentials must be stored using the secure config mechanisms defined in
  the security specifications.

## Monitoring
Kafka integration must expose:
- Producer error counts
- Consumer lag and offset progress
- Ingest apply rate and error counts

These metrics are included in the Prometheus metrics reference:
`docs/specifications/operations/PROMETHEUS_METRICS_REFERENCE.md`

## Implementation Checklist (Beta)
- [ ] Implement Kafka producer (CDC broadcaster)
- [ ] Implement Kafka consumer (CDC ingest)
- [ ] Add DDL event channel (optional)
- [ ] Wire audit Kafka output (existing security spec)
- [ ] Persist offsets in catalog metadata
- [ ] Add metrics for producer/consumer and CDC lag
- [ ] Provide Kafka Connect source/sink adapters (optional)
